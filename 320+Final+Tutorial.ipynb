{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "This tutorial will introduce a few basic methods for processing financial data, with a focus on individual stock growth. Financial data is an area where many individuals and corporations take interest in with the hope of predicting market movement and direction in order to make informed trade decisions for profit. As with many data science topics, the goal would be to utilize the data and interpret in a meaningful way to identify and/or predict trends.     \n",
    "  \n",
    "  \n",
    "# Tutorial Content: \n",
    "Due to my extremely limited knowledge of the financial field, this tutorial will have a focus on acquiring and visualizing the data, with a bit of data analysis and machine learning to illustrate some concepts. In this tutorial, a few of the main python libraries we will be utilizing are [Pandas](http://), [BeautifulSoup](http://), and [pyplot](http://).  \n",
    "  \n",
    "We will be utilizing data scraped from Bloomberg's stocks section: https://www.bloomberg.com/markets/stocks, as well as historical stock data from Google finance: https://www.google.com/finance. While the data from Bloomberg only displays a fairly limited number of stocks representative of each region, the methods from this tutorial can be extended for usage on other websites with moderate tweaking.  \n",
    "  \n",
    "In this tutorial, we will cover the basic concepts of the data science pipeline, including but not limited to the following subjects:  \n",
    "* Data Curation, Parsing, and Management\n",
    "* Exploratory Data Analysis\n",
    "* Hypothesis Testing\n",
    "* Machine Learning\n",
    "\n",
    "# Setup:\n",
    "Before we get started with anything else, we need to import a few python libraries that will be critical to this tutorial. Below is the code provided to set up the notebook. Don't worry too much about the imports for now since they will be explained as they come up in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import html5lib, re, requests\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optimize notebook width, row, and columns for for visual clarity\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Curation, Parsing, and Management\n",
    "Before we can even begin doing any sort of analysis, we must first acquire data to perform analysis on. That brings us to the first step within the data science pipeline: data curation. Data curation is very broad term that can refer to the management of data anywhere from the creation of it and its storage, to its archival and deletion. Clearly this seems quite expansive but we can take it in this context to essentially mean gathering data and storing it in a suitable medium.  \n",
    "  \n",
    "For this tutorial, we will be utilizing the 'requests' library to perform HTTP get requests on websites for web scraping (to retrieve its HTML). To begin, we want to scrape the stock data from Bloomberg's Americas, Europe, Africa, Middle East, and Asian Pacific markets. We can retrieve the website using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use requests to get a connection to the URL\n",
    "temp = requests.get('https://www.bloomberg.com/markets/stocks/world-indexes/americas')\n",
    "temp2 = requests.get('https://www.bloomberg.com/markets/stocks/world-indexes/europe-africa-middle-east')\n",
    "temp3 = requests.get('https://www.bloomberg.com/markets/stocks/world-indexes/asia-pacific')\n",
    "\n",
    "# The above line of code will give us a connection to the website but we can't quite do anything with that so what \n",
    "# we'll want to do is utilize the 'BeautifulSoup' library to parse the data into lxml:\n",
    "root = BeautifulSoup(temp.content,'lxml')\n",
    "root2 = BeautifulSoup(temp2.content,'lxml')\n",
    "root3 = BeautifulSoup(temp3.content,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the website, we'll want to look through the HTML to find out where the relevant data of the website will be contained. By analyzing Bloomberg's HTML, we can see that each row of stock data is contained within a '<tr>' tag with the class of 'data-table-row.' Since we know all of this, we can utilize 'BeautifulSoup' findAll() function in order to parse through the data and return all of the stock data rows. Now that we have this data we'll want to store it in a 'Pandas' dataframe since it provides an efficient data structure that possesses high-performance and allows for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;tr class=\"data-table-row\"&gt; &lt;td class=\"data-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;tr class=\"data-table-row\"&gt; &lt;td class=\"data-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;tr class=\"data-table-row\"&gt; &lt;td class=\"data-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;tr class=\"data-table-row\"&gt; &lt;td class=\"data-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;tr class=\"data-table-row\"&gt; &lt;td class=\"data-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>&lt;tr class=\"data-table-row\"&gt; &lt;td class=\"data-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>&lt;tr class=\"data-table-row\"&gt; &lt;td class=\"data-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>&lt;tr class=\"data-table-row\"&gt; &lt;td class=\"data-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>&lt;tr class=\"data-table-row\"&gt; &lt;td class=\"data-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>&lt;tr class=\"data-table-row\"&gt; &lt;td class=\"data-ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  data\n",
       "0    <tr class=\"data-table-row\"> <td class=\"data-ta...\n",
       "1    <tr class=\"data-table-row\"> <td class=\"data-ta...\n",
       "2    <tr class=\"data-table-row\"> <td class=\"data-ta...\n",
       "3    <tr class=\"data-table-row\"> <td class=\"data-ta...\n",
       "4    <tr class=\"data-table-row\"> <td class=\"data-ta...\n",
       "..                                                 ...\n",
       "212  <tr class=\"data-table-row\"> <td class=\"data-ta...\n",
       "213  <tr class=\"data-table-row\"> <td class=\"data-ta...\n",
       "214  <tr class=\"data-table-row\"> <td class=\"data-ta...\n",
       "215  <tr class=\"data-table-row\"> <td class=\"data-ta...\n",
       "216  <tr class=\"data-table-row\"> <td class=\"data-ta...\n",
       "\n",
       "[217 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use BeautifulSoup's findAll function to find all table rows\n",
    "table = root.findAll(\"tr\", {\"class\" : \"data-table-row\" })\n",
    "table2 = root2.findAll(\"tr\", {\"class\" : \"data-table-row\" })\n",
    "table3 = root3.findAll(\"tr\", {\"class\" : \"data-table-row\" })\n",
    "\n",
    "# Use pandas to import the table from above step into a dataframe named df\n",
    "df = pd.DataFrame(table+table2+table3,columns=['data'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now we have a Pandas dataframe consisting of rows of HTML containing the information of stock data. That doesn't seem to look very readable or provide any sort of insight into our data. Now what? This is where data parsing comes into play.\n",
    "\n",
    "### Data Parsing and Management\n",
    "We will need to use the 're' library, short for regular expressions, in order to capture the data that is hidden in between the HTML tags. We need to craft a regex 'formula' in order to match the HTML relevant to what we're looking for. What the code below will do is iterate through each row in the table and split them. After that, it will iterate through the split rows to match the regex and extract the name and 1 month percent change elements between the appropiate tags. \n",
    "\n",
    "When all that is done, we're left with a dataframe possessing 3 columns, the initial 'data' column, the stock 'name' column, and the 1 month projection '1_month' column. Since we have already extracted the data we care about, we can now delete the initial column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>1_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NASDAQ TRANSPORTATION IX</td>\n",
       "      <td>+1.54%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S&amp;amp;P 600 SMALLCAP INDEX</td>\n",
       "      <td>-0.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RUSSELL 2000 INDEX</td>\n",
       "      <td>-0.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NASDAQ BIOTECH INDEX</td>\n",
       "      <td>+1.66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOW JONES INDUS. AVG</td>\n",
       "      <td>+0.68%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>MSE Top 20 Index</td>\n",
       "      <td>-2.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>MSCI ASIA APEX 50</td>\n",
       "      <td>+8.27%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Laos Composite Index</td>\n",
       "      <td>-2.86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>PSEi - PHILIPPINE SE IDX</td>\n",
       "      <td>+3.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>SRI LANKA COLOMBO ALL SH</td>\n",
       "      <td>+4.94%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name 1_month\n",
       "0       NASDAQ TRANSPORTATION IX   +1.54%\n",
       "1     S&amp;P 600 SMALLCAP INDEX   -0.49%\n",
       "2             RUSSELL 2000 INDEX   -0.06%\n",
       "3           NASDAQ BIOTECH INDEX   +1.66%\n",
       "4           DOW JONES INDUS. AVG   +0.68%\n",
       "..                            ...     ...\n",
       "212             MSE Top 20 Index   -2.64%\n",
       "213            MSCI ASIA APEX 50   +8.27%\n",
       "214         Laos Composite Index   -2.86%\n",
       "215     PSEi - PHILIPPINE SE IDX   +3.12%\n",
       "216     SRI LANKA COLOMBO ALL SH   +4.94%\n",
       "\n",
       "[217 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regex to catch the name of the company between the div tags\n",
    "regname = re.compile('div.*>(.*)</div')\n",
    "# Regex to catch the percent change between the td tags\n",
    "regpct = re.compile('td.*>(.*)</td')\n",
    "\n",
    "# Iterate through each row in the table\n",
    "for index,row in df.iterrows():\n",
    "    temprow = str(row['data']).split('> <')\n",
    "\n",
    "    name = regname.match(temprow[4]).group(1)\n",
    "    pct = regpct.match(temprow[10]).group(1)\n",
    "\n",
    "    df.set_value(index,'name',name)\n",
    "    df.set_value(index,'1_month',pct)\n",
    "\n",
    "# Delete unneeded column\n",
    "del df['data']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "With our data all nice and sorted, it is time to begin some exploratory data analysis! We're going to want to analyze the trends and directions in the various world markets right? Unfortunately, the data provided from Bloomberg is not even close to significant enough to perform any sort of statistical analysis or exploratory data analysis. The Bloomberg data only provides current market data and unless you only want to find basic information such as the average market growth for the various regions, you're going to need historic data. \n",
    "\n",
    "Don't worry though, as all of what we did thus far has not gone to waste since it introduced the concept and basics of web scraping, data parsing using BeautifulSoup, and data storage. For the rest of this tutorial, we will be using historic stock data obtained through Google finance. A nifty feature from pandas allows us to directly import a CSV file into a pandas dataframe. \n",
    "\n",
    "This code below will pull historic stock data from the past 5 years for Apple (AAPL) and IBM (IBM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-May-17</td>\n",
       "      <td>145.10</td>\n",
       "      <td>147.20</td>\n",
       "      <td>144.96</td>\n",
       "      <td>146.58</td>\n",
       "      <td>33602943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28-Apr-17</td>\n",
       "      <td>144.09</td>\n",
       "      <td>144.30</td>\n",
       "      <td>143.27</td>\n",
       "      <td>143.65</td>\n",
       "      <td>20860358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27-Apr-17</td>\n",
       "      <td>143.92</td>\n",
       "      <td>144.16</td>\n",
       "      <td>143.31</td>\n",
       "      <td>143.79</td>\n",
       "      <td>14246347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26-Apr-17</td>\n",
       "      <td>144.47</td>\n",
       "      <td>144.60</td>\n",
       "      <td>143.38</td>\n",
       "      <td>143.68</td>\n",
       "      <td>20041241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-Apr-17</td>\n",
       "      <td>143.91</td>\n",
       "      <td>144.90</td>\n",
       "      <td>143.87</td>\n",
       "      <td>144.53</td>\n",
       "      <td>18871501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>7-May-12</td>\n",
       "      <td>80.21</td>\n",
       "      <td>81.82</td>\n",
       "      <td>80.18</td>\n",
       "      <td>81.35</td>\n",
       "      <td>115073217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>4-May-12</td>\n",
       "      <td>82.44</td>\n",
       "      <td>82.62</td>\n",
       "      <td>80.74</td>\n",
       "      <td>80.75</td>\n",
       "      <td>132497421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>3-May-12</td>\n",
       "      <td>84.36</td>\n",
       "      <td>84.49</td>\n",
       "      <td>82.90</td>\n",
       "      <td>83.12</td>\n",
       "      <td>97637057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2-May-12</td>\n",
       "      <td>82.89</td>\n",
       "      <td>83.91</td>\n",
       "      <td>82.69</td>\n",
       "      <td>83.71</td>\n",
       "      <td>106906016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>1-May-12</td>\n",
       "      <td>83.56</td>\n",
       "      <td>85.25</td>\n",
       "      <td>83.03</td>\n",
       "      <td>83.16</td>\n",
       "      <td>152749513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date    Open    High     Low   Close     Volume\n",
       "0      1-May-17  145.10  147.20  144.96  146.58   33602943\n",
       "1     28-Apr-17  144.09  144.30  143.27  143.65   20860358\n",
       "2     27-Apr-17  143.92  144.16  143.31  143.79   14246347\n",
       "3     26-Apr-17  144.47  144.60  143.38  143.68   20041241\n",
       "4     25-Apr-17  143.91  144.90  143.87  144.53   18871501\n",
       "...         ...     ...     ...     ...     ...        ...\n",
       "1253   7-May-12   80.21   81.82   80.18   81.35  115073217\n",
       "1254   4-May-12   82.44   82.62   80.74   80.75  132497421\n",
       "1255   3-May-12   84.36   84.49   82.90   83.12   97637057\n",
       "1256   2-May-12   82.89   83.91   82.69   83.71  106906016\n",
       "1257   1-May-12   83.56   85.25   83.03   83.16  152749513\n",
       "\n",
       "[1258 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple = pd.read_csv(\"http://www.google.com/finance/historical?cid=22144&startdate=May+1%2C+2012&\" + \n",
    "                    \"enddate=May+1%2C+2017&num=30&ei=bAoeWdHiDseOep_UvNgC&output=csv\")\n",
    "ibm = pd.read_csv(\"http://www.google.com/finance/historical?cid=18241&startdate=May+1%2C+2012&\" + \n",
    "                  \"enddate=May+1%2C+2017&num=30&ei=vAoeWeDuJITBeKing8gG&output=csv\")\n",
    "apple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, all the data is neatly imported! Now we want to convert the 'Date' column into a value that can be utilized in plotting and calculations because the current representation for the stored value is a string, which isn't compatible with what we'll be doing. To accomplish this task, we will be utilizing pandas' to_datetime() function which will convert the values into a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>145.10</td>\n",
       "      <td>147.20</td>\n",
       "      <td>144.96</td>\n",
       "      <td>146.58</td>\n",
       "      <td>33602943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04-28</td>\n",
       "      <td>144.09</td>\n",
       "      <td>144.30</td>\n",
       "      <td>143.27</td>\n",
       "      <td>143.65</td>\n",
       "      <td>20860358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-04-27</td>\n",
       "      <td>143.92</td>\n",
       "      <td>144.16</td>\n",
       "      <td>143.31</td>\n",
       "      <td>143.79</td>\n",
       "      <td>14246347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-26</td>\n",
       "      <td>144.47</td>\n",
       "      <td>144.60</td>\n",
       "      <td>143.38</td>\n",
       "      <td>143.68</td>\n",
       "      <td>20041241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-25</td>\n",
       "      <td>143.91</td>\n",
       "      <td>144.90</td>\n",
       "      <td>143.87</td>\n",
       "      <td>144.53</td>\n",
       "      <td>18871501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>2012-05-07</td>\n",
       "      <td>80.21</td>\n",
       "      <td>81.82</td>\n",
       "      <td>80.18</td>\n",
       "      <td>81.35</td>\n",
       "      <td>115073217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>2012-05-04</td>\n",
       "      <td>82.44</td>\n",
       "      <td>82.62</td>\n",
       "      <td>80.74</td>\n",
       "      <td>80.75</td>\n",
       "      <td>132497421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2012-05-03</td>\n",
       "      <td>84.36</td>\n",
       "      <td>84.49</td>\n",
       "      <td>82.90</td>\n",
       "      <td>83.12</td>\n",
       "      <td>97637057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2012-05-02</td>\n",
       "      <td>82.89</td>\n",
       "      <td>83.91</td>\n",
       "      <td>82.69</td>\n",
       "      <td>83.71</td>\n",
       "      <td>106906016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>2012-05-01</td>\n",
       "      <td>83.56</td>\n",
       "      <td>85.25</td>\n",
       "      <td>83.03</td>\n",
       "      <td>83.16</td>\n",
       "      <td>152749513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date    Open    High     Low   Close     Volume\n",
       "0    2017-05-01  145.10  147.20  144.96  146.58   33602943\n",
       "1    2017-04-28  144.09  144.30  143.27  143.65   20860358\n",
       "2    2017-04-27  143.92  144.16  143.31  143.79   14246347\n",
       "3    2017-04-26  144.47  144.60  143.38  143.68   20041241\n",
       "4    2017-04-25  143.91  144.90  143.87  144.53   18871501\n",
       "...         ...     ...     ...     ...     ...        ...\n",
       "1253 2012-05-07   80.21   81.82   80.18   81.35  115073217\n",
       "1254 2012-05-04   82.44   82.62   80.74   80.75  132497421\n",
       "1255 2012-05-03   84.36   84.49   82.90   83.12   97637057\n",
       "1256 2012-05-02   82.89   83.91   82.69   83.71  106906016\n",
       "1257 2012-05-01   83.56   85.25   83.03   83.16  152749513\n",
       "\n",
       "[1258 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple['Date'] = pd.to_datetime(apple['Date'], infer_datetime_format=True)\n",
    "ibm['Date'] = pd.to_datetime(ibm['Date'], infer_datetime_format=True)\n",
    "\n",
    "apple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data, we can use python's 'pyplot' library\n",
    "\n",
    "Notice how despite the amount of rows and columns being about 6x as big as our previous dataset, all the graphing and regression methods are executed almost instantaneously due to pandas' efficiency. Pandas' ability to handle large amounts of data is a key reason why it is utilized so heavily in the field of data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the size of the plot to be created (width x length)\n",
    "plt.figure(figsize=(18,9))\n",
    "\n",
    "# We utilize pyplot to graph apple's date as the x-axis vs apple's stock open as the y-axis\n",
    "# The third argument for the plot, 'ro' indicates the color and symbol of each point\n",
    "plt.plot(apple['Date'], apple['Open'], 'ro')\n",
    "\n",
    "plt.plot(ibm['Date'], ibm['Open'], 'bo')\n",
    "\n",
    "# Here we create a legend for the plot using our custom values and set the location\n",
    "# to be automatically placed in the 'best' optimal spot\n",
    "plt.legend(['Apple [AAPL]','IBM [IBM]'],loc=\"best\")\n",
    "\n",
    "# Plot functions for setting the basic labels: title, x-axis, y-axis\n",
    "plt.title('Apple vs. IBM Stock Open 2012-2017')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Cost per share ($)')\n",
    "\n",
    "coef = np.polyfit(range(1258), ibm['Open'], 1)\n",
    "pol = np.poly1d(coef)\n",
    "ys = pol(range(1258))    \n",
    "plt.plot(ibm['Date'],ys)\n",
    "\n",
    "coef2 = np.polyfit(range(1258), apple['Open'], 1)\n",
    "pol2 = np.poly1d(coef2)\n",
    "ys2 = pol2(range(1258))    \n",
    "plt.plot(apple['Date'],ys2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "sd  \n",
    "\n",
    "# Machine Learning\n",
    "sd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_descent(X, y, T, alpha):\n",
    "    m, n = X.shape # m = #examples, n = #features\n",
    "    theta = np.zeros(n) # initialize parameters\n",
    "    f = np.zeros(T) # track loss over time\n",
    "    for i in range(T):\n",
    "        # loss for current parameter vector theta\n",
    "        f[i] = 0.5*np.linalg.norm(X.dot(theta) - y)**2\n",
    "        # compute steepest ascent at f(theta)\n",
    "        g = X.T.dot(X.dot(theta) - y)\n",
    "        # step down the gradient\n",
    "        theta = theta - alpha*g\n",
    "    return theta, f\n",
    "\n",
    "x, y = make_regression(n_samples=100, n_features=1, n_informative=1, \n",
    "                        random_state=0, noise=35) \n",
    "m, n = np.shape(x)\n",
    "x = np.c_[ np.ones(m), x] # insert column\n",
    "alpha = 0.01 # learning rate\n",
    "theta,f = grad_descent(x, y, 1000, alpha)\n",
    "for i in range(x.shape[1]):\n",
    "    y_predict = theta[0] + theta[1]*x \n",
    "pylab.plot(x[:,1],y,'o')\n",
    "pylab.plot(x,y_predict,'k-')\n",
    "pylab.show()\n",
    "print (\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
